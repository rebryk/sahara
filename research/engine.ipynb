{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import tqdm\n",
    "import pickle\n",
    "import openai\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List\n",
    "from openai import embeddings_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION = \"\"\n",
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = ORGANIZATION\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CACHE_PATH = \"data/embedding_cache.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    EMBEDDING_CACHE = pd.read_pickle(EMBEDDING_CACHE_PATH)\n",
    "except FileNotFoundError:\n",
    "    EMBEDDING_CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cache():\n",
    "    with open(EMBEDDING_CACHE_PATH, \"wb\") as embedding_cache_file:\n",
    "        pickle.dump(EMBEDDING_CACHE, embedding_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_from_string(string: str, engine: str = \"text-similarity-babbage-001\", save: bool = True) -> List:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, engine) not in EMBEDDING_CACHE.keys():\n",
    "        EMBEDDING_CACHE[(string, engine)] = utils.get_embedding(string, engine)\n",
    "        \n",
    "        if save:\n",
    "            save_cache()\n",
    "\n",
    "    return EMBEDDING_CACHE[(string, engine)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_documents(\n",
    "    query: str,\n",
    "    documents: List[dict],\n",
    "    engine: str= \"text-similarity-babbage-001\",\n",
    "    count: int = 1\n",
    ") -> List[int]:\n",
    "    query_embedding = embedding_from_string(query, engine=engine)\n",
    "    embeddings = [embedding_from_string(doc[\"content\"], engine=engine) for doc in documents]\n",
    "    distances = utils.distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "    indices_of_nearest_neighbors = utils.indices_of_nearest_neighbors_from_distances(distances)\n",
    "\n",
    "    results = []\n",
    "    count = min(count, len(documents))\n",
    "    for idx in indices_of_nearest_neighbors[:count]:\n",
    "        document = documents[idx]\n",
    "        results.append({\n",
    "            \"id\": document[\"id\"],\n",
    "            \"index\": idx,\n",
    "            \"content\": document[\"content\"],\n",
    "            \"distance\": distances[idx]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")\n",
    "df = df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPPING = {\n",
    "    \"mau\": \"monthly active users\",\n",
    "    \"dau\": \"daily active user\",\n",
    "    \"eth\": \"ethereum\",\n",
    "    \"sol\": \"solana\",\n",
    "    \"bnb\": \"binance\",\n",
    "    \"lens\": \"lens protocol\",\n",
    "    \"ens\": \"ethereum name service\",\n",
    "    \"tvl\": \"total value locked\",\n",
    "    \"avg\": \"average\",\n",
    "    \"mean\": \"average\",\n",
    "    \"tx\": \"transcation\",\n",
    "    \"$\": \"dollars\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text: str) -> str:\n",
    "    words = re.split(\"[,.!?:() ]\", text)\n",
    "    words = [w.lower() for w in words if len(w) > 0]\n",
    "    words = [MAPPING.get(w, w) for w in words]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_desc(visualizations: str) -> str:\n",
    "    result = \"\"\n",
    "    visualizations = json.loads(visualizations)\n",
    "    \n",
    "    for v in visualizations:\n",
    "        if len(v[\"name\"].split()) <= 1:\n",
    "            continue\n",
    "        \n",
    "        if v[\"type\"] != \"counter\":\n",
    "            continue\n",
    "\n",
    "        desc = f\"use {v['options']['counterColName']} for {v['name'].lower()}\"\n",
    "        desc = normalize(desc)\n",
    "        \n",
    "        if len(result) > 0:\n",
    "            result += \", \"\n",
    "        \n",
    "        result += desc\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for i, query in df.iterrows():\n",
    "    content = f\"{query['name']} {query['description']}\".strip()\n",
    "    content = normalize(content)\n",
    "\n",
    "    desc = extract_desc(query[\"visualizations\"])\n",
    "    if len(desc) > 0:\n",
    "        content += f\" ({desc})\"\n",
    "    \n",
    "    content = content.strip()\n",
    "    contents.append(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"content\"] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, query in df.iterrows():\n",
    "    documents.append({\"id\": query.id, \"content\": query.content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_CACHE = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3638/3638 [07:01<00:00,  8.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for document in tqdm.tqdm(documents):\n",
    "    embedding_from_string(document[\"content\"], engine=\"text-similarity-babbage-001\", save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompt.txt\", \"w\") as f:\n",
    "    f.write(\"WITH all_trades AS\\n(SELECT\\nt.block_time,\\nCASE --filling in missing addresses for coins. Maybe not worth keeping up, some are memes.\\nWHEN t.token_a_address::text = '\\\\x95ad61b0a150d79219dcf64e1e6cc01f0b64c4ce' THEN 'SHIB'\\nWHEN t.token_a_address::text = '\\\\x761d38e5ddf6ccf6cf7c55759d5210750b5d60f3' THEN 'ELON'\\nWHEN t.token_a_address::text = '\\\\x3301ee63fb29f863f2333bd4466acb46cd8323e6' THEN 'AKITA'\\nWHEN t.token_a_address::text = 'x5218e472cfcfe0b64a064f055b43b4cdc9efd3a6' THEN 'eRSDL'\\nWHEN t.token_a_address::text = '\\\\x1453dbb8a29551ade11d89825ca812e05317eaeb' THEN 'TEND'\\nWHEN t.token_a_address::text = '\\\\x35a532d376ffd9a705d0bb319532837337a398e7' THEN 'WDOGE'\\nWHEN t.token_a_address::text = '\\\\x17ef75aa22dd5f6c2763b8304ab24f40ee54d48a' THEN 'RVP'\\nWHEN t.token_a_address::text = '\\\\x6dea81c8171d0ba574754ef6f8b412f2ed88c54d' THEN 'LQTY'\\nWHEN t.token_a_address::text = '\\\\x3832d2f059e55934220881f831be501d180671a7' THEN 'renDOGE'\\nWHEN t.token_a_address::text = '\\\\xf16e4d813f4dcfde4c5b44f305c908742de84ef0' THEN 'ETH2x ADL'\\nWHEN t.token_a_address::text = '\\\\x77fba179c79de5b7653f68b5039af940ada60ce0' THEN 'FORTH'\\nWHEN t.token_a_address::text = '\\\\xf65b5c5104c4fafd4b709d9d60a185eae063276c' THEN 'TRU'\\nWHEN t.token_a_address::text = '\\\\xc7283b66eb1eb5fb86327f08e1b5816b0720212b' THEN 'TRIBE'\\nWHEN t.token_a_address::text = '\\\\xc8807f0f5ba3fa45ffbdc66928d71c5289249014' THEN 'ISP'\\nELSE COALESCE(t.token_a_symbol,t.token_a_address::text)\\nEND AS token_a,\\nCASE --filling in missing addresses for coins. Maybe not worth keeping up, some are memes.\\nWHEN t.token_b_address::text = '\\\\x95ad61b0a150d79219dcf64e1e6cc01f0b64c4ce' THEN 'SHIB'\\nWHEN t.token_b_address::text = '\\\\x761d38e5ddf6ccf6cf7c55759d5210750b5d60f3' THEN 'ELON'\\nWHEN t.token_b_address::text = '\\\\x3301ee63fb29f863f2333bd4466acb46cd8323e6' THEN 'AKITA'\\nWHEN t.token_b_address::text = 'x5218e472cfcfe0b64a064f055b43b4cdc9efd3a6' THEN 'eRSDL'\\nWHEN t.token_b_address::text = '\\\\x1453dbb8a29551ade11d89825ca812e05317eaeb' THEN 'TEND'\\nWHEN t.token_b_address::text = '\\\\x35a532d376ffd9a705d0bb319532837337a398e7' THEN 'WDOGE'\\nWHEN t.token_b_address::text = '\\\\x17ef75aa22dd5f6c2763b8304ab24f40ee54d48a' THEN 'RVP'\\nWHEN t.token_b_address::text = '\\\\x6dea81c8171d0ba574754ef6f8b412f2ed88c54d' THEN 'LQTY'\\nWHEN t.token_b_address::text = '\\\\x3832d2f059e55934220881f831be501d180671a7' THEN 'renDOGE'\\nWHEN t.token_b_address::text = '\\\\xf16e4d813f4dcfde4c5b44f305c908742de84ef0' THEN 'ETH2x ADL'\\nWHEN t.token_b_address::text = '\\\\x77fba179c79de5b7653f68b5039af940ada60ce0' THEN 'FORTH'\\nWHEN t.token_b_address::text = '\\\\xf65b5c5104c4fafd4b709d9d60a185eae063276c' THEN 'TRU'\\nWHEN t.token_b_address::text = '\\\\xc7283b66eb1eb5fb86327f08e1b5816b0720212b' THEN 'TRIBE'\\nWHEN t.token_b_address::text = '\\\\xc8807f0f5ba3fa45ffbdc66928d71c5289249014' THEN 'ISP'\\nELSE COALESCE(t.token_b_symbol,t.token_b_address::text)\\nEND AS token_b\\n,t.token_a_amount\\n,t.token_b_amount\\n,t.exchange_contract_address -- exchange_contract_address is the v3 LP Position\\n,t.tx_hash -- transaction addr (for spot checks)\\n,t.usd_amount --TODO: Find some way to pull in USD of any missing tokens\\n,c.fee/1e6 AS fee_tier --Converting fee tier to a multiplier (i.e. for 0.3%, 3000 -> 0.003)\\n,t.usd_amount*(c.fee/1e6) AS fees_collected_usd --USD Amount * %fee = fees collected\\nFROM dex.\\\"trades\\\" t\\n\\nLEFT JOIN uniswap_v3.\\\"Factory_call_createPool\\\" c --1. Join With Pool creation on LP contract address to pull fee tier\\nON t.exchange_contract_address = c.output_pool\\nWHERE t.project = 'Uniswap'\\nAND t.version = '3'\\nAND t.block_time >= (DATE_TRUNC('day',CURRENT_TIMESTAMP) - '6 days'::INTERVAL)\\nAND t.block_time >= '05-05-2021 17:00' -- Uni v3 launch date/hr\\nORDER BY t.block_time DESC\\n),\\n\\nsum_pairs AS( -- Using this to decide which trading pair order to pick (i.e WBTC/USDC vs USDC/WBTC). We'll pick the one with the greatest usd volume.\\n--starting set\\nSELECT \\nCONCAT(token_a,'/',token_b) AS forward_pair -- Trading pair\\n,CONCAT(token_b,'/',token_a) AS backward_pair -- For joining buys/sells later\\n,SUM(usd_amount) AS sum_usd\\nFROM all_trades\\nGROUP BY 1,2\\n),\\n\\nfinal_trading_pairs AS( --building the mapping table for pairs\\nSELECT\\na.forward_pair, a.backward_pair,\\na.sum_usd,b.sum_usd,\\nCASE\\nWHEN RIGHT(a.backward_pair,4)='WETH' THEN a.forward_pair -- if the backward version ends in WETH, do forward\\nWHEN RIGHT(a.forward_pair,4)='WETH' THEN a.backward_pair\\nWHEN RIGHT(a.backward_pair,4)='WBTC' THEN a.forward_pair\\nWHEN RIGHT(a.forward_pair,4)='WBTC' THEN a.backward_pair\\nWHEN (a.sum_usd >= b.sum_usd) -- when a > b\\nOR b.sum_usd IS NULL -- or if backward usd is null\\nTHEN a.forward_pair -- select forward\\nELSE a.backward_pair --else select backward in every other circumstance\\nEND AS trading_pair\\n\\nFROM sum_pairs a\\nLEFT JOIN sum_pairs b\\nON a.forward_pair = b.backward_pair\\n),\\nv3_trades AS (  --clean table of trades to work with for building visuals\\nSELECT\\natx.block_time,\\nCOALESCE(f1.trading_pair,f2.trading_pair) AS trading_pair, --pick the mapped trading pair (see joins)\\natx.usd_amount,\\natx.fee_tier,\\natx.fees_collected_usd,\\natx.tx_hash\\nFROM all_trades atx\\n\\nLEFT JOIN final_trading_pairs f1 --check if the pair maps with the forward version\\nON CONCAT(atx.token_a,'/',atx.token_b) = f1.forward_pair\\nLEFT JOIN final_trading_pairs f2 --check if the pair maps with the backward version\\nON CONCAT(atx.token_a,'/',atx.token_b) = f2.backward_pair\\n)\\n\\nSELECT\\ntrading_pair\\n,COUNT(trading_pair) AS num_trades\\n,SUM(usd_amount) AS total_usd\\nFROM v3_trades\\nWHERE usd_amount IS NOT NULL -- Eventually see if Dune adds new tokens, or we need to manually\\nGROUP BY trading_pair--, fee_tier\\nORDER BY SUM(usd_amount) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, query in df.iterrows():\n",
    "    documents.append({\n",
    "        \"id\": query.id,\n",
    "        \"content\": f\"{query['name']} {query.description or ''}\".strip()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"number of ens registered accounts\"\n",
    "recommendations = find_best_documents(query, documents, count=20)\n",
    "recommendations = [r for r in recommendations if r[\"distance\"] < 0.25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\n",
    "for doc in recommendations:\n",
    "    raw_doc = df.iloc[doc[\"index\"]]\n",
    "    \n",
    "    prompt += f\"/* Write SQL query: {raw_doc['name']} */\\n\"\n",
    "    prompt += raw_doc[\"query\"].strip()\n",
    "    prompt += \"\\n\\n\"\n",
    "\n",
    "    if len(prompt) >= 6000:\n",
    "        break\n",
    "\n",
    "prompt += f\"/* Write SQL query: {query} */\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openai.Completion.create(\n",
    "  model=\"code-davinci-002\",\n",
    "  prompt=prompt,\n",
    "  max_tokens=512,\n",
    "  temperature=0,\n",
    "  stop=[\"STOP\", \"/* Write SQL\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT COUNT(*) FROM (\n",
      "    SELECT DISTINCT(owner) FROM (\n",
      "        SELECT * FROM ethereumnameservice.\"ENSRegistry_evt_NewOwner\"\n",
      "        UNION\n",
      "        SELECT * FROM ethereumnameservice.\"ENSRegistryWithFallback_evt_NewOwner\"\n",
      "    ) as rr\n",
      ") as r\n"
     ]
    }
   ],
   "source": [
    "completion = result.choices[0].text.strip()\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sahara')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fe31cd8a6ccf64627f122f4639693caed8bd5d61a632260ef19ae32aaa3b5e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
